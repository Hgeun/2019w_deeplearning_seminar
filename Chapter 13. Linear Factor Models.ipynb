{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 13. Linear Factor Models\n",
    "13.0 Intro  \n",
    "13.1 Probabilistic PCA and Factor Analysis  \n",
    "13.2 Independent Component Analysis (ICA)  \n",
    "13.3 Slow Feature Analysis  \n",
    "13.4 Sparse Coding  \n",
    "13.5 Manifold Interpretation of PCA  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13.0 Intro\n",
    "- 많은 딥러닝 연구에서 입력데이터의 확률모델을 만들어 사용\n",
    "- 원칙적으로, 다른 변수들이 주어진 환경에서 어떤 변수들을 예측하기 위해 probabilistic inference(확률론적 추론)을 사용가능\n",
    "- 또한, latent variables (잠재 변수) $\\textbf{h}$ 를 갖고, 입력데이터의 확률모델을 다음과 같이 표현 ; 볼드체=벡터  \n",
    "$$ p_{model}(\\textbf{x})=\\mathbb{E}_{\\textbf{h}p_{model}}(\\textbf{x}|\\textbf{h}) $$  \n",
    "- 위처럼 잠재변수에 의해 데이터 확률모델을 다르게 표현 가능\n",
    "- 잠재변수로 표현하는 distributed representations은 deep feed-forward network와 recurrent network에서 봤던 ***표현학습의 장점들*** 을 획득 가능\n",
    "  \n",
    "  -------------------------------\n",
    "   \n",
    "- 잠재 변수를 가진 가장 단순한 확률모델 : **linear factor models**\n",
    "\t- blocks of mixture models 설계시 이용\n",
    "\t- 더 크고 깊은 확률 모델 설계시 이용\n",
    "\t- generative models 설계시 필요한 기본 접근법 제시\n",
    "\n",
    "-  $\\textbf{h}$의 linear transformation에 noise를 추가하여 $\\textbf{x}$를 생성하는\n",
    " ***stochastic linear decoder function*** 을 이용하여 linear factor model 정의\n",
    "- 이 모델들은 ***simple joint distribution*** 을 가진 ***explanatory factor*** 들을 우리가 찾을 수 있도록 도움\n",
    "- linear decoder 이용의 단순함은 이 모델들을 확장적으로 학습되는 첫번째 잠재변수 모델로 만든다.(??? 도대체 뭔뜻)\n",
    "- linear factor model은 다음과 같은 데이터 생성 프로세스를 설명\n",
    "\t1. explanatory factor $\\textbf{h}$ 를 distribution $\\textbf{h} \\sim p(\\textbf{h})$로부터 샘플링\n",
    "\t $p(\\textbf{h})$ 는 factorial distribution,  샘플링하기 쉬움 \n",
    "\t  $$p(\\textbf{h})= \\prod_{i=1} p(h_i) $$\n",
    "\t2. 주어진 factor $\\textbf{h}$ 에서 관찰가능한 실수값 변수들로 샘플링\n",
    "\t노이즈는 가우시안 + diagonal (independent across dimensions)\n",
    "\t$$ \\textbf{x}=\\textbf{Wh} + \\textbf{b} + noise$$\n",
    "- 이후 소개될 probabilistic PCA나 다른 linear factor model들은 \n",
    "\t$$ \\textbf{h} \\sim p(\\textbf{h})  \\\\\\\\\n",
    "\t  \\textbf{x}=\\textbf{Wh} + \\textbf{b} + noise $$\n",
    "\t위 두 식의 특별한 케이스\n",
    "\t노이즈 분포가 다르거나, h의 model의 prior가 다르거나"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13.1 Probabilistic PCA and Factor Analysis\n",
    "\n",
    "- Factor analysis에서 잠재변수의 prior는 unit variance 가우시안  \n",
    "$$ \\textbf{h} \\sim \\mathcal{N}(\\textbf{h};\\textbf{0},\\textbf{I}) $$\n",
    "- 반면, $\\textbf{h}$가 주어졌을 때, observed 변수들 $x_{i}$는 conditionally independent(조건부 독립)로 가정  \n",
    "(참고 : http://norman3.github.io/prml/docs/chapter08/2.html )\n",
    "$$ \\psi = diag(\\sigma ^2) $$\n",
    "$$ \\sigma ^2 = [\\sigma ^2 _1 , \\dotsc ,\\sigma ^2 _n]^\\top $$\n",
    "    위와 같이 noise가 diagonal covariance 가우시안 분포임을 가정, 각 변수 별로.\n",
    "- 잠재변수 $\\textbf{h}$의 역할은 다른 변수들 $x_{i}$ 사이 ***dependecies capture***\n",
    "- 즉, data vector $\\textbf{x}$가 multivariate noraml random variable  \n",
    "$$ \\textbf{x} \\sim \\mathcal{N}(\\textbf{x};\\textbf{b},\\textbf{WW}^\\top + \\psi) $$\n",
    "----------------------------------\n",
    "- PCA : principal components analysis  \n",
    "    (참고 : https://wikidocs.net/7646 [공돌이의 수학정리노트]) \n",
    "- probabilistic PCA는 multivariate가 아닌 동일한 variation인 경우 ($\\textbf{z}$는 $noise$)\n",
    "\t$$ \\sigma^2 = \\sigma^2 _1 = \\dotsc = \\sigma^2 _n $$\n",
    "\t$$ \\textbf{x} = \\textbf{Wh} + \\textbf{b} + \\sigma\\textbf{z} , \\textbf{z} \\sim \\mathcal{N}(\\textbf{z};\\textbf{0},\\textbf{I})$$  \n",
    "- Probabilistic PCA 모델의 장점\n",
    "    - data가 가지고 있는 대부분의 variation들을 $\\textbf{h}$에 의해 잡아냄\n",
    "- reconstruction error $\\sigma ^2$\n",
    "- $\\sigma$가 0에 수렴 -> Sharp, https://www.desmos.com/calculator/0x3rpqtgrx [그래프]\n",
    "$$  \\textbf{x} = \\textbf{Wh} + \\textbf{b} + \\sigma\\textbf{z} $$ \n",
    "$$  \\textbf{x} - \\textbf{b} = \\textbf{Wh} $$\n",
    "- data에 노이즈가 없다는 전제\n",
    "\t- data가 정확하게 hyperplane에 대해 clustering되어야 함\n",
    "\t- 가능성이 떨어지는 전제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13.2 Independent Component Analysis (ICA)\n",
    "\n",
    "- Linear factor를 모델링\n",
    "\t- linear factor : 많은 underlying signal들 속에서 observed signal을 분리를 목표로 함\n",
    "\t\t- underlying signals : observed signal(data)을 만들기 위해 같이 추가되고 scale된 신호들\n",
    "\t\t- 서로 fully independent\n",
    "- 주로 ICA는 signal들을 분리시키기 위한 분석 tool로 사용\n",
    "- ICA의 많은 변형들 존재\n",
    "- user에 의해 $p(\\textbf{h})$가 고정\n",
    "\t- non-Gaussian\n",
    "\t- $p(\\textbf{h})$가 Gaussian이면 $\\textbf{W}$를 확정할 수 없음(여러 개의 값)\n",
    "- model을 $\\textbf{x}=\\textbf{Wh}$로 생성\n",
    "- $\\textbf{x}$에 대한 확률분포 $p(\\textbf{x})$를 nonlinear하게  \n",
    "![image](https://user-images.githubusercontent.com/33209778/50593943-de33aa80-0edd-11e9-930a-f9972e90e013.png)\n",
    "\n",
    "- Nonlinear independent components estimation(NICE)\n",
    "\t- encoder stage에서 invertible transformations을 쌓아 올린 형태\n",
    "        - 각 transforamtion의 Jacobian의 determinant 계산을 효율적으로 할 수 있게\n",
    "- independent subspace analysis\n",
    "\t- feature들을 그룹별로 학습하도록\n",
    "        - feature 그룹간에는 discouraged, 그룹내에서는 allowed한 통계적 dependence를 가지게끔"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13.3 Slow Feature Analysis(SFA)\n",
    "\n",
    "- invariant feature를 학습하기 위해 time signal로 부터의 정보를 이용한 linear factor model\n",
    "------------------------\n",
    "- Slowness principle\n",
    "- scenes의 중요 특성들은 개별적 measurements에 비해 느리게 변화\n",
    "\t- 예시 : zebra가 달리는 장면\n",
    "\t\t- pixel 값은 white - black 매우 빠르게 변화\n",
    "\t\t- zebra자체는 매우 느리게 변화\n",
    "- 즉, 시간에 대해 느리게 변화하는 feature들을 학습할 모델\n",
    "- slowness principle은 gradient descent로 학습한 미분가능한 모델이면 적용 가능\n",
    "\t- cost function에 특정 term을 적용\n",
    "\t- ![image](https://user-images.githubusercontent.com/33209778/50596044-379fd780-0ee6-11e9-9593-8a1fc366730d.png)\n",
    "\t- $\\lambda$ slowness regularization 조절 파라미터\n",
    "\t- $t$ time index\n",
    "\t- $f$ feature extractor\n",
    "\t- $L$ loss function, 둘 사이 distance\n",
    "\n",
    "-----------------\n",
    "- SFA는 slowness principle의 특정적 - 효율적 적용\n",
    "\t- linear feature extractor에 적용\n",
    "\t- closed form(linear algebra)으로 train 가능\n",
    "- input 과 feature 사이의 linear map을 정의\n",
    "\t- feature space에 대한 prior $p(\\textbf{h})$ 를 정의하는 건 아님\n",
    "\t- input space에서 $p(\\textbf{x})$를 정의하는 건 아님\n",
    "- SFA 알고리즘은 linear transformation $f(\\textbf{x}; \\theta )$을 정의하고, 최적화\n",
    "\t- ![image](https://user-images.githubusercontent.com/33209778/50599784-61f79200-0ef2-11e9-8d68-b1e102345878.png)\n",
    "\t- 2 constraints\n",
    "\t\t- ![image](https://user-images.githubusercontent.com/33209778/50599888-aaaf4b00-0ef2-11e9-9b33-3a76e517a38b.png)\n",
    "\t\t- 학습된 feature가 0의 mean을 갖도록 하는 constraint는 unique solution을 갖기위해 필요\n",
    "\t\t- 학습된 feature의 variance가 unit(1)이도록 하는 constraint는 모든 feature가 0이 되지 않기 위해 필요\n",
    "- SFA feature들도 순서 존재, 가장 slowest 하면 first feature\n",
    "- 학습된 feature들끼리는 서로 linearly decorrelated 해야만 함\n",
    "\t- ![image](https://user-images.githubusercontent.com/33209778/50600221-cb2bd500-0ef3-11e9-991b-30b49d7ef8a1.png)\n",
    "\t- 같은 시간상에서 모든 feature들 사이의 expectation이 0\n",
    "\t- 위 constraint가 없으면, slowest signal을 모든 feature가 학습\n",
    "-  SFA 적용 전에 data에 대해 nonlinear basis expansion을 적용하여, nonlinear한 feature를 학습\n",
    "\t- $x$를 quadratic basis expansion -> data사이의 차이를 더 크게 볼 수 있게\n",
    "- 이론적으로 nonlinear setting에서도 SFA가 어떤 feature들을 학습할지 예측 가능\n",
    "\t- 구성 공간의 관점에서 environment의 dynamics에 대한 정보 필요\n",
    "\t\t- 카메라의 속도, 위치에 대한 확률분포\n",
    "\t- underlying factor들이 어떻게 변화하는지에 대한 정보 필요\n",
    "\t\t- 이 factor들을 표현하는 최적의 함수들에 대해 분석적으로 solve 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13.4 Sparse Coding\n",
    "\n",
    "- unsupervised feature learning 과 feature extraction mechanism으로 학습된 linear factor model\n",
    "- 엄격히는 $\\textbf{h}$ 값에 대한 과정인 sparse coding과 model 학습과 설계 과정인 sparse modeling으로 나뉘어지지만, 보통 sparse coding으로 같이 묶어서 씀\n",
    "- data $\\textbf{x}$의 복원을 위해 noise가 추가된 linear decoder 사용\n",
    "- linear factor들이 isotropic precision $\\beta$가 있는 가우시안 noise를 갖는다고 가정\n",
    "\t- ![image](https://user-images.githubusercontent.com/33209778/50602284-4c866600-0efa-11e9-84b0-0cb4878e23a0.png)\n",
    "- $p(\\textbf{h})$는 0근처에서 sharp peak을 갖는 분포로 선택\n",
    "\t- 보통 factorized Laplace, Cauchy 또는  factorized Student t-distribution 사용\n",
    "\t- ![image](https://user-images.githubusercontent.com/33209778/50602493-c3bbfa00-0efa-11e9-905d-c96cf073a4e6.png)\n",
    "\t- factorized Student t-distribution\n",
    "\t![image](https://user-images.githubusercontent.com/33209778/50602518-d9312400-0efa-11e9-9360-77a2ce518d2e.png)\n",
    "- maximum likelihood로 sparse coding을 training 하는 것은 어려움\n",
    "\t- data를 encoding하고,\n",
    "\t- encoding된 data를 더 잘 reconstruction하기 위해 decoder를 training하는 것을 반복\n",
    "\n",
    "- sparse coding에서 encoder는 최적화 알고리즘\n",
    "\t- 가장 가능성이 큰 값을 찾기 위한\n",
    "\t- ![image](https://user-images.githubusercontent.com/33209778/50603122-c4ee2680-0efc-11e9-9ae2-0d1196893e3a.png)\n",
    "![image](https://user-images.githubusercontent.com/33209778/50603110-bc95eb80-0efc-11e9-8ebc-9a01fcd53f48.png)\n",
    "\t- $\\textbf{h}$에 대한 $L^1$ norm 에 의해 sparse $\\textbf{h}^*$ 발생\n",
    "\t- $\\lambda$와 $\\beta$에 의해 $\\textbf{h}$와 $\\textbf{W}$ 각각의 minimization 조절\n",
    "- non-parametric encoder를 이용한 sparse coding\n",
    "\t- log-prior와 reconstruction error의 combination을 최소화 가능\n",
    "\t- encoder에 generalization error가 없음\n",
    "\t\t- parametric encoder는 $\\textbf{x}$와 $\\textbf{h}$사이를 generalize하며 mapping을 학습\n",
    "\t\t\t- 못보던 data에 대해선 제대로 작동하지 않음\n",
    "\t\t- classifier를 위한 feature extractor로 좋은 성능\n",
    "\n",
    "\t- 단점1 : 반복적으로 알고리즘을 실행해야 해서 시간이 많이 걸림\n",
    "\t- 단점2 : back-propagate 어려움"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13.5 Manifold Interpretation of PCA\n",
    "\n",
    "- PCA를 포함한 linear factor models, factor analysis는 \"manifold를 학습하는 것\"으로 해석\n",
    "- ![image](https://user-images.githubusercontent.com/33209778/50604456-61ff8e00-0f02-11e9-98b1-ea420b892993.png)\n",
    "- ( manifold 참고 : http://t-robotics.blogspot.com/2015/12/pca.html#.XCz-MlwzaUk )\n",
    "- encoder\n",
    "![image](https://user-images.githubusercontent.com/33209778/50605284-40ec6c80-0f05-11e9-94d3-762109766a2b.png)\n",
    "- decoder\n",
    "![image](https://user-images.githubusercontent.com/33209778/50605296-4ba70180-0f05-11e9-948a-ed23b2916df1.png)\n",
    "-reconstruction error\n",
    "![image](https://user-images.githubusercontent.com/33209778/50605313-56619680-0f05-11e9-8a62-303bf4e2624c.png)\n",
    "- covariance matrix\n",
    "![image](https://user-images.githubusercontent.com/33209778/50605328-624d5880-0f05-11e9-9e21-281453db05cb.png)\n",
    "\n",
    "- PCA에서 $C$의 eigenvector가 $\\textbf{W}$의 열벡터, 크기 순 나열\n",
    "- ![image](https://user-images.githubusercontent.com/33209778/50605594-0931f480-0f06-11e9-9451-1089e09e322b.png)\n",
    "\t- $C$의 rank 가 $d$이면 eigenvalue $\\lambda_{d+1}$ 에서 $\\lambda_D$가 0이고, reconstruction error가 0\n",
    "\t- $D$ data의 차원\n",
    "\t- $d$ $\\textbf{h}$의 차원"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<딥러닝 스터디 공지>\n",
    "- 매주 목요일 오전 8시30분 시작\n",
    "- 깃헙 마크다운파일로 발표자료 정리\n",
    "\n",
    "※지각시 5천원\n",
    "지각 벌금은 마지막 스터디 발표 후 \n",
    "커피 또는 간식비로 사용\n",
    "\n",
    "- 특별한 경우가 아니면 k201로 계속 빌릴 예정\n",
    "- 다음 발표자는?\n",
    "- 정리할 포맷을 다시 정해보기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
