{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 13. Linear Factor Models\n",
    "13.0 Intro  \n",
    "13.1 Probabilistic PCA and Factor Analysis  \n",
    "13.2 Independent Component Analysis (ICA)  \n",
    "13.3 Slow Feature Analysis  \n",
    "13.4 Sparse Coding  \n",
    "13.5 Manifold Interpretation of PCA  \n",
    "\n",
    "## 13.0 Intro\n",
    "- 많은 딥러닝 연구에서 입력데이터의 확률모델을 만들어 사용\n",
    "- 원칙적으로, 다른 변수들이 주어진 환경에서 어떤 변수들을 예측하기 위해 probabilistic inference(확률론적 추론)을 사용가능\n",
    "- 또한, latent variables (잠재 변수) $\\textbf{h}$ 를 갖고, 입력데이터의 확률모델을 다음과 같이 표현 ; 볼드체=벡터\n",
    "  $$ p_{model}(\\textbf{x})=\\mathop{\\mathbb{E}} _{\\textbf{h}p_{model}}(\\textbf{x}|\\textbf{h}) $$  \n",
    "- 위처럼 잠재변수에 의해 데이터 확률모델을 다르게 표현 가능\n",
    "- 잠재변수로 표현하는 distributed representations은 deep feed-forward network와 recurrent network에서 봤던 ***표현학습의 장점들*** 을 획득 가능\n",
    "  \n",
    "  -------------------------------\n",
    "   \n",
    "- 잠재 변수를 가진 가장 단순한 확률모델 : **linear factor models**\n",
    "\t- blocks of mixture models 설계시 이용\n",
    "\t- 더 크고 깊은 확률 모델 설계시 이용\n",
    "\t- generative models 설계시 필요한 기본 접근법 제시\n",
    "\n",
    "-  $\\textbf{h}$ 의 linear transformation에 noise를 추가하여 $\\textbf{x}$ 를 생성하는\n",
    " ***stochastic linear decoder function*** 을 이용하여 linear factor model 정의\n",
    "- 이 모델들은 ***simple joint distribution*** 을 가진 ***explanatory factor*** 들을 우리가 찾을 수 있도록 도움\n",
    "- linear decoder 이용의 단순함은 이 모델들을 확장적으로 학습되는 첫번째 잠재변수 모델로 만든다.(??? 도대체 뭔뜻)\n",
    "- linear factor model은 다음과 같은 데이터 생성 프로세스를 설명\n",
    "\t1. explanatory factor $\\textbf{h}$ 를 distribution $\\textbf{h} \\sim p(\\textbf{h})$로부터 샘플링\n",
    "\t $p(\\textbf{h})$ 는 factorial distribution,  샘플링하기 쉬움 \n",
    "\t  $$p(\\textbf{h})= \\prod_{i=1} p(h_i) $$\n",
    "\t2. 주어진 factor $\\textbf{h}$ 에서 관찰가능한 실수값 변수들로 샘플링\n",
    "\t노이즈는 가우시안 + diagonal (independent across dimensions)\n",
    "\t$$ \\textbf{x}=\\textbf{Wh} + \\textbf{b} + noise$$\n",
    "- 이후 소개될 probabilistic PCA나 다른 linear factor model들은 \n",
    "\t$$ \\textbf{h} \\sim p(\\textbf{h})  $$\n",
    "\t$$ \\textbf{x}=\\textbf{Wh} + \\textbf{b} + noise $$\n",
    "\t위 두 식의 특별한 케이스\n",
    "\t노이즈 분포가 다르거나, h의 model의 prior가 다르거나\n",
    "\t\n",
    "## 13.1 Probabilistic PCA and Factor Analysis\n",
    "- PCA : principal components analysis\n",
    "- Factor analysis에서 잠재변수의 prior는 unit variance 가우시안\n",
    "$$ \\textbf{h} \\sim \\mathcal{N}(\\textbf{h};\\textbf{0},\\textbf{I}) $$\n",
    "- 반면, $\\textbf{h}$가 주어졌을 때, observed 변수들 $x_{i}$는 conditionally independent로 가정\n",
    "- \n",
    "\n",
    "## 13.2 Independent Component Analysis (ICA)\n",
    "\n",
    "## 13.3 Slow Feature Analysis\n",
    "\n",
    "## 13.4 Sparse Coding\n",
    "\n",
    "## 13.5 Manifold Interpretation of PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
